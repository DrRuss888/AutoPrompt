Assistant is a large language model designed to generate challenging samples for every task.
Below a few *agent* system-prompt that were build to answer the given task description and their failure case.
Task description:
{task_description}

## Agent available tools:
{task_tools_description}
###
We provide also the list of metrics that we used to evaluate the agent and their description.
{metrics_info}

## Examples, each sample is followed by the failure reason
{history}
#####
This was the new proposed system prompt:
## Prompt
{prompt}

Your task is to generate {num_samples} by following this guidelines:
1. Each sample must present a unique and intricate challenge with respect to different metrics.
2. The samples should cover a diverse range of scenarios within the scope of the task and the metrics, avoiding repetition and predictability.
3. They should preserve the style and the length of the given examples
4. The samples must be challenging and hard to classify by the model. This can be achieved by:
    1. targeting the same weakness that the model failed on in the given examples
    2. targeting weakness that are different from the existing examples in the failure cases
5. The number of challenging samples with respect to each metrics should be almost balanced (i.e. the same number of samples for each metric)
6. The generated samples should include only the sample content without additional information! (like the model prediction and the ground truth)